{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic example of using deep learning to see handwritten text from MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#tf =  2,490,119 lines of code\n",
    "\n",
    "tf.__version__\n",
    "#!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.[1][2] The database is also widely used for training and testing in the field of machine learning.[3][4] It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments.[5] Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels -wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images of handwritten digits 0-9, 28 x 28 images\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n",
      "  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n",
      "  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n",
      "  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n",
      "  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n",
      "  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
      "  0.33153488 0.11664776 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.20500962\n",
      "  0.33153488 0.24625638 0.00291174 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01622378\n",
      "  0.24897876 0.32790981 0.10191096 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n",
      "  0.04089933 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n",
      "  0.245396   0.05882702 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02333517 0.12857881 0.32549285\n",
      "  0.41390126 0.40743158 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32161793\n",
      "  0.41390126 0.54251585 0.20001074 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n",
      "  0.41390126 0.45100715 0.00625034 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n",
      "  0.40899334 0.39653769 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04117838 0.16813739\n",
      "  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n",
      "  0.12760592 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n",
      "  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.37491383 0.56222061\n",
      "  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n",
      "  0.17428513 0.01425695 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.92705966 0.82698729\n",
      "  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#this is a tensor. \"a mathematical object analogous to but more general than a vector, \n",
    "#represented by an array of components that are functions of the coordinates of a space.\"\n",
    "\n",
    "\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[13])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2655 - accuracy: 0.9214\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1100 - accuracy: 0.9658\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0733 - accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x137961090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok I want to leave the output above so I am redoing some of the code below\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)\n",
    "#the neural net is not maximizing accuracy, it is minimizing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note:\n",
    "#ReLU stands for rectified linear unit, and is a type of activation function\n",
    "#The activation function is a mathematical “gate” in between the input feeding \n",
    "#the current neuron and its output going to the next layer. It can be as simple \n",
    "#as a step function that turns the neuron output on and off, depending on a rule \n",
    "#or threshold.\n",
    "#Softmax function outputs a vector that represents the probability distributions of a\n",
    "#list of potential outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0913 - accuracy: 0.9719\n",
      "0.09131877019759267 0.9719\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the basics of keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: understanding_humans.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('understanding_humans.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('understanding_humans.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
